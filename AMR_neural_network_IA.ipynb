{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d21d63-cf9e-4fa9-b0b0-61c94505b880",
   "metadata": {},
   "source": [
    "## 1. Importar módulos necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb03ebc-42ea-4b89-9460-7a4e0fd17c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 11:48:21.096841: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-03 11:48:21.099252: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-03 11:48:21.106773: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738579701.120029   21500 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738579701.123909   21500 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-03 11:48:21.136942: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 1. Importar módulos necesarios\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e8bde-811f-448b-b93e-128493b54bdf",
   "metadata": {},
   "source": [
    "## 1.1 Comprobar si los paquetes se han cargado correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b349020c-2305-4e18-b063-aee52c19e399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "Eager execution: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1738580062.986832   21500 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m210\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m11\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">221</span> (884.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m221\u001b[0m (884.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">221</span> (884.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m221\u001b[0m (884.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 1.1 Comprobar si los paquetes se han cargado correctamente\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Eager execution:\", tf.executing_eagerly())\n",
    "\n",
    "# Verificar si Keras funciona\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(20,)),  # Define explícitamente la capa de entrada\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d7b08-5990-41f5-b4bb-a2d39bdf008d",
   "metadata": {},
   "source": [
    "## 2. Definir la estructura de los archivos de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97b6c4-88d9-4ab0-bb48-cb0447049254",
   "metadata": {},
   "source": [
    "## 2. Definir la estructura de los archivos de entrada\n",
    "\n",
    "**Archivo de Entrenamiento (\"training_data.csv\")**:\n",
    "- Columnas 1 a 160: Genes de resistencia.\n",
    "  - Valor: \"\" (vacío) si la cepa es wild type para ese gen.\n",
    "  - Valor: Cadena de mutaciones separadas por comas, e.g., \"A123T,V456G\", si hay mutaciones.\n",
    "- Columnas 161 a 163: \"IMI_MIC\", \"AZT_MIC\", \"FEP_MIC\" (valores numéricos, pero no se usan directamente en el modelo).\n",
    "- Columnas 164 a 166: \"IMI_eval\", \"AZT_eval\", \"FEP_eval\" (evaluación clínica: \"sensitive\" o \"resistant\").\n",
    "- Columna 167: \"danger_profile\" (por ejemplo, \"sensitive\", \"MDR\", \"XDR\").\n",
    "\n",
    "**Archivo de Test (\"test_data.csv\")**:\n",
    "- Columnas 1 a 160: Genes de resistencia (mismo formato).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e9a67-74ce-489d-8264-23fe71fe030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 3. Cargar y preprocesar los datos\n",
    "# Leer el archivo de entrenamiento (1200 cepas)\n",
    "train_df = pd.read_csv('training_data.csv')\n",
    "\n",
    "# Suponemos que las primeras 160 columnas son genes de resistencia\n",
    "gene_cols = train_df.columns[:68]\n",
    "\n",
    "# --- Preprocesamiento de los genes: codificación multi-hot por gen ---\n",
    "# La idea es, para cada gen, construir un vocabulario de mutaciones (observadas en el set de entrenamiento)\n",
    "# y, para cada celda, crear un vector binario indicando la presencia de cada mutación.\n",
    "def build_gene_vocab(df, gene_columns):\n",
    "    vocab = {}\n",
    "    for col in gene_columns:\n",
    "        mutations = set()\n",
    "        for val in df[col]:\n",
    "            if pd.isna(val) or val.strip() == \"\":\n",
    "                continue\n",
    "            # Dividir las mutaciones por coma\n",
    "            for mut in val.split(\",\"):\n",
    "                mut = mut.strip()\n",
    "                if mut:\n",
    "                    mutations.add(mut)\n",
    "        # Ordenamos el vocabulario para tener un orden fijo\n",
    "        vocab[col] = sorted(list(mutations))\n",
    "    return vocab\n",
    "\n",
    "gene_vocab = build_gene_vocab(train_df, gene_cols)\n",
    "\n",
    "print('Archivo de lectura: train_df:')\n",
    "print(train_df)\n",
    "\n",
    "print('Columnas con genes de resistencia: gene_cols:')\n",
    "print(gene_cols)\n",
    "\n",
    "print('Vocabulario de mutaciones: gene_vocab')\n",
    "print(gene_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc27c21-df81-4cb3-9de2-588a283ee5b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# ## 3. Cargar y preprocesar los datos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Leer el archivo de entrenamiento (1200 cepas)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Suponemos que las primeras 160 columnas son genes de resistencia\u001b[39;00m\n\u001b[1;32m      7\u001b[0m gene_cols \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39mcolumns[:\u001b[38;5;241m160\u001b[39m]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/AMR_prediction_IA-eVOz-xu8/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/AMR_prediction_IA-eVOz-xu8/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/AMR_prediction_IA-eVOz-xu8/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/AMR_prediction_IA-eVOz-xu8/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/AMR_prediction_IA-eVOz-xu8/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training_data.csv'"
     ]
    }
   ],
   "source": [
    "# Función para codificar una fila: para cada gen, si la celda está vacía se retorna un vector de ceros,\n",
    "# si tiene mutaciones, se asigna 1 en las posiciones correspondientes al vocabulario.\n",
    "def encode_gene_row(row, gene_columns, vocab):\n",
    "    features = []\n",
    "    for col in gene_columns:\n",
    "        gene_voc = vocab[col]\n",
    "        vec = np.zeros(len(gene_voc), dtype=int)\n",
    "        cell = row[col]\n",
    "        if pd.isna(cell) or cell.strip() == \"\":\n",
    "            # Wild type: vector de ceros\n",
    "            pass\n",
    "        else:\n",
    "            mutations = [m.strip() for m in cell.split(\",\") if m.strip() != \"\"]\n",
    "            for mut in mutations:\n",
    "                if mut in gene_voc:\n",
    "                    idx = gene_voc.index(mut)\n",
    "                    vec[idx] = 1\n",
    "        # Agregar el vector para este gen a la lista de características\n",
    "        features.extend(vec.tolist())\n",
    "    return features\n",
    "\n",
    "# Aplicar la codificación a cada fila para los genes en el set de entrenamiento\n",
    "X_train = train_df.apply(lambda row: encode_gene_row(row, gene_cols, gene_vocab), axis=1)\n",
    "X_train = np.array(X_train.tolist())\n",
    "\n",
    "# Preparar las etiquetas:\n",
    "# Usaremos las evaluaciones clínicas de MIC para 3 antibióticos (IMI, AZT, FEP) y el perfil de peligrosidad.\n",
    "# Suponemos que las columnas de evaluación están etiquetadas como \"IMI_eval\", \"AZT_eval\", \"FEP_eval\"\n",
    "antibiotic_cols = ['IMI_eval', 'AZT_eval', 'FEP_eval']\n",
    "y_antibiotics = train_df[antibiotic_cols]\n",
    "\n",
    "# Para cada antibiótico, codificamos las etiquetas (\"sensitive\" o \"resistant\") en formato binario.\n",
    "antibiotic_encoders = {}\n",
    "y_antibiotics_encoded = {}\n",
    "for col in antibiotic_cols:\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y_antibiotics[col].astype(str))\n",
    "    antibiotic_encoders[col] = le\n",
    "    # Convertimos a formato one-hot (aunque en clasificación binaria se puede usar una sola neurona con sigmoide)\n",
    "    y_antibiotics_encoded[col] = to_categorical(y_enc)\n",
    "\n",
    "# Para el perfil de peligrosidad (por ejemplo: \"sensitive\", \"MDR\", \"XDR\")\n",
    "le_profile = LabelEncoder()\n",
    "y_profile = le_profile.fit_transform(train_df['danger_profile'].astype(str))\n",
    "y_profile_cat = to_categorical(y_profile)\n",
    "\n",
    "# Para este ejemplo, definiremos las salidas de la siguiente forma:\n",
    "# - Para cada antibiótico: salida binaria (usaremos 1 neurona con sigmoide)  \n",
    "#   Por ello, en lugar de one-hot, convertiremos las etiquetas a 0/1.\n",
    "y_antibiotics_binary = {}\n",
    "for col in antibiotic_cols:\n",
    "    # Supongamos que \"resistant\" es 1 y \"sensitive\" es 0\n",
    "    y_binary = (y_antibiotics[col].astype(str).str.lower() == \"resistant\").astype(int)\n",
    "    y_antibiotics_binary[col] = y_binary.values.reshape(-1, 1)\n",
    "\n",
    "# Consolidamos las salidas en un diccionario para el modelo multi-salida\n",
    "y_train = {\n",
    "    'IMI': y_antibiotics_binary['IMI_eval'],\n",
    "    'AZT': y_antibiotics_binary['AZT_eval'],\n",
    "    'FEP': y_antibiotics_binary['FEP_eval'],\n",
    "    'profile': y_profile_cat\n",
    "}\n",
    "\n",
    "# Como nuestros datos de genes ya están en formato binario (multi-hot), la estandarización puede no ser necesaria.\n",
    "X_train_input = X_train  # Forma: (1200, total_features) donde total_features = sum(len(vocab[gene]) for each gene)\n",
    "\n",
    "# Cargar los datos de test (200 cepas), que solo tienen las 160 columnas de genes.\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "# Asegurarse de que los test tengan las mismas columnas y procesarlos de la misma forma\n",
    "for col in gene_cols:\n",
    "    test_df[col] = test_df[col].astype(str)\n",
    "X_test = test_df.apply(lambda row: encode_gene_row(row, gene_cols, gene_vocab), axis=1)\n",
    "X_test_input = np.array(X_test.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a6b1d-1351-4c3b-9dc7-c1d8f141184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 4. Definir el modelo de redes neuronales multi-salida\n",
    "#\n",
    "# Utilizaremos el API funcional de Keras para construir un modelo que tenga dos tipos de salidas:\n",
    "# - Tres salidas binarias para la evaluación de IMI, AZT y FEP.\n",
    "# - Una salida multiclase para el perfil de peligrosidad (3 clases).\n",
    "input_layer = Input(shape=(X_train_input.shape[1],), name='input')\n",
    "x = Dense(256, activation='relu')(input_layer)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# Salidas para antibióticos (una neurona cada uno con activación sigmoide)\n",
    "out_IMI = Dense(1, activation='sigmoid', name='IMI')(x)\n",
    "out_AZT = Dense(1, activation='sigmoid', name='AZT')(x)\n",
    "out_FEP = Dense(1, activation='sigmoid', name='FEP')(x)\n",
    "\n",
    "# Salida para perfil de peligrosidad (3 clases, activación softmax)\n",
    "out_profile = Dense(3, activation='softmax', name='profile')(x)\n",
    "\n",
    "# Definir el modelo con entradas y salidas\n",
    "model = Model(inputs=input_layer, outputs=[out_IMI, out_AZT, out_FEP, out_profile])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'IMI': 'binary_crossentropy',\n",
    "                    'AZT': 'binary_crossentropy',\n",
    "                    'FEP': 'binary_crossentropy',\n",
    "                    'profile': 'categorical_crossentropy'},\n",
    "              metrics={'IMI': 'accuracy',\n",
    "                       'AZT': 'accuracy',\n",
    "                       'FEP': 'accuracy',\n",
    "                       'profile': 'accuracy'})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2628a3-6d88-4fca-be42-c3ffd8b7d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Entrenar el modelo\n",
    "history = model.fit(X_train_input, y_train,\n",
    "                    epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8241b7b0-47e0-48a7-83d3-7bc348f78771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 6. Evaluar el modelo en el set de test\n",
    "#\n",
    "# Para el set de test, se usa únicamente la información de las mutaciones para predecir:\n",
    "# - La probabilidad de resistencia para cada antibiótico (se puede usar un umbral, p.ej. 0.5)\n",
    "# - El perfil de peligrosidad (la clase con mayor probabilidad)\n",
    "predictions = model.predict(X_test_input)\n",
    "\n",
    "# Para las salidas binarias, aplicar umbral 0.5\n",
    "pred_IMI = (predictions[0] > 0.5).astype(int)\n",
    "pred_AZT = (predictions[1] > 0.5).astype(int)\n",
    "pred_FEP = (predictions[2] > 0.5).astype(int)\n",
    "# Para el perfil, se toma la clase de mayor probabilidad\n",
    "pred_profile = np.argmax(predictions[3], axis=1)\n",
    "pred_profile_labels = le_profile.inverse_transform(pred_profile)\n",
    "\n",
    "# Mostrar algunas predicciones (ejemplo: perfil de peligrosidad)\n",
    "print(\"Predicción del perfil de peligrosidad para las primeras 5 cepas del set de test:\")\n",
    "print(pred_profile_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
